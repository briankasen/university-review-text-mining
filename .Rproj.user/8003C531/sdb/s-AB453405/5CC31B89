{
    "collab_server" : "",
    "contents" : "library(tm)\n\n#Create Corpus\n# docs <- Corpus(DirSource(\"C:/Users/joel1/OneDrive/2017 2018 Spring A/MRKT 845 Advanced Marketing Analytics/Project 2 Text Mining/project_2_text_mining/School Reviews\"))\ndocs <- Corpus(DirSource(\"reviews/\"))\n\n# Also create an unaltered copy of docs\ndocs_orig <- docs\n\n# verify number of documents in the corpa\ndocs\n\n#inspect a particular document. here, we picked one of the longest written review, one from purdue\nwriteLines(as.character(docs[[3]]))\n\n# create the toSpace content transformer\ntoSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, \" \", x))})\n\n# docs <- tm_map(docs, toSpace, \"-\")\n# docs <- tm_map(docs, toSpace, \":\")\n\n# Remove punctuation marks\ndocs <- tm_map(docs, removePunctuation)\n\n# Check the result of the transformation applied using the purdue review\nwriteLines(as.character(docs[[3]]))\n\n# Remove non-standard punctuation\n# docs <- tm_map(docs, toSpace, \"'\")\n# docs <- tm_map(docs, toSpace, \"'\")\n# docs <- tm_map(docs, toSpace, \" -\")\n\n#alternatively, view printout of more documents at a time, by lines, for quicker inspection\nstrwrap(docs[1:20])\n\n#Transform to lower case (need to wrap in content_transformer)\ndocs <- tm_map(docs,content_transformer(tolower))\n\n#check result of the transformation applied\nstrwrap(docs[1:20])\n\n#Strip digits (std transformation, so no need for content_transformer)\ndocs <- tm_map(docs, removeNumbers)\n\n#check again the result of the transformation applied\nstrwrap(docs[1:20])\n\n# writeLines(as.character(docs[[30]]))\n\n#remove stopwords using the standard list in tm\n# but first create doc with the stop words still preserved\n# placed the removal of general stopwords after removal of university names which used stopwords\ndocs_w_stop_words <- docs\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\n\n#check result of transformation again\nstrwrap(docs[1:20])\n\n# writeLines(as.character(docs[[30]]))\n\n# remove additional words found in higher frequency but are not associative\ndocs <- tm_map(docs, removeWords, c(\"the\", \"like\", \"also\", \"said\", \"often\", \"go\", \"theres\", \"just\", \"say\", \"get\", \"say\", \"can\", \"see\", \"will\", \"one\", \"doand\", \"dont\", \"u\", \"youll\", \"im\", \"ive\", \"although\", \"via\", \"etc\", \"youre\", \"within\", \"either\", \"even\", \"let\", \"tell\", \"wholl\", \"though\", \"shall\", \"couldnt\", \"wouldnt\", \"shouldnt\", \"didnt\", \"doesnt\", \"us\", \"throughout\", \"however\", \"andor\", \"pu\"))\n\n#check result of transformation again\nstrwrap(docs[1:20])\n\n# load library\n# library(SnowballC)\n# Stem document\n# docs <- tm_map(docs,stemDocument)\n# writeLines(as.character(docs[[30]]))\n\n# Build a Document Term Matrix\ndtm <- DocumentTermMatrix(docs)\n\ndtm\n\n# inspect a small portion of the dtm\ninspect(dtm[3:3,1:50])\n\n# get frequency of words in the corpus\nfreq <- colSums(as.matrix(dtm))\n# freq\n\n#length should be total number of terms\nlength(freq)\n\n# create sort order (descending)\nord <- order(freq,decreasing=TRUE)\n# ord\n\n#inspect most frequently occurring terms\nfreq[head(ord)]\n\n#inspect least frequently occurring terms\nfreq[tail(ord)]\n\n# define threshold variables so they can be used for the aggregate dtm and the institution dtms further down in the script\ndtm_minimum_word_length = 3\ndtm_maximum_word_length = 50\ndtm_minimum_doc_frequency = 1\ndtm_maximum_doc_frequency = 700\n\n# Here we have told R to include only those words that within our doc frequency thresholds\n# We have also enforced  lower and upper limit to length of the words included (between 3 and 50 characters)\ndtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(dtm_minimum_word_length, dtm_maximum_word_length), bounds = list(global = c(dtm_minimum_doc_frequency,dtm_maximum_doc_frequency))))\ndtmr\n\nfreqr <- colSums(as.matrix(dtmr))\n\n#length should be total number of terms\nlength(freqr)\n\n#create sort order (asc)\nordr <- order(freqr,decreasing=TRUE)\n\n#inspect most frequently occurring terms\nfreqr[head(ordr)]\n\n#inspect least frequently occurring terms\nfreqr[tail(ordr)]\n\n# let's take get a list of terms that occur at least a  100 times in the entire corpus\nfindFreqTerms(dtmr,lowfreq=100)\n\n# term correlations: correlation is a quantitative measure of the co-occurrence of words in multiple documents\ncorrelation_threshold <- .1\nfindAssocs(dtmr,\"northwesternuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"universityofmichiganannarbor\",correlation_threshold)\nfindAssocs(dtmr,\"universityofwisconsinmadison\",correlation_threshold)\nfindAssocs(dtmr,\"universityofillinoisaturbanachampaign\",correlation_threshold)\nfindAssocs(dtmr,\"purdueuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"pennsylvaniastateuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"ohiostateuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"universityofminnesotatwincities\",correlation_threshold)\nfindAssocs(dtmr,\"michiganstateuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"universityofmarylandcollegepark\",correlation_threshold)\nfindAssocs(dtmr,\"indianauniversitybloomington\",correlation_threshold)\nfindAssocs(dtmr,\"universityofiowa\",correlation_threshold)\nfindAssocs(dtmr,\"rutgersuniversitynewbrunswick\",correlation_threshold)\nfindAssocs(dtmr,\"universityofnebraskalincoln\",correlation_threshold)\n\n#Now let us perform editing of misspelled words, words that should split up, and words that should be combined\n# after running the correlation and frequency steps, where we found additional words that we felt should be edited\n\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"on campus\", replacement = \"oncampus\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"off campus\", replacement = \"offcampus\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"professorprofess\", replacement = \"professor\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"professs\", replacement = \"professor\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"professorta\", replacement = \"professor\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"professors\", replacement = \"professor\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"students\", replacement = \"student\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"sciences\", replacement = \"science\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" campus beautiful \", replacement = \" beautiful campus \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"succeed\", replacement = \"success\")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" university \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" northwestern \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" michigan \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" ann arbor \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" wisconsin \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" madison \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" illinois \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" urbana \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" champaign \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" uiuc \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" purdue \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" purdues \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" penn \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" penn state \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" ohio \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" ohio state \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" columbus \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" minnesota \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" twin cities \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" maryland \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" college park \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" indiana \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" bloomington \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" iowa \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" iowa city \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" rutgers \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" new brunswick \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" nebraska \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" lincoln \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"illini\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"spartan\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"badger\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"husker\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"buckeye\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"bucks\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"boiler\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"boilermaker\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"hoosier\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \"blue\", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" u \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" i \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" m \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" w \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" iu \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" unl \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" msu \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" umn \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" umd \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" uw \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" osu \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" um \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" don t \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" tell \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" spending \", replacement = \" spend \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" start \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" day \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" donÃ \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" come \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" knowing \", replacement = \" know \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" itll  \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" applying \", replacement = \" applying \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" put \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" puts \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" youÃ \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" lakes \", replacement = \" lake \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" havent \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" saw \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" got \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" set \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" instead \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" either \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" thon \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" brought \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" maybe \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" brings \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" suggest \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" didnt \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" thus \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" therefore \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" gone \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" used \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" becomes \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" perhaps \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" wish \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" next \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" recent \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" due \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" without \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" end \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" honestly \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" isnt \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" five \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" youre \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" lot \", replacement = \" lots \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" somehow \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" recently \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" around \", replacement = \" \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" missionsplit \", replacement = \" mission split \")\ndocs <- tm_map(docs, content_transformer(gsub), pattern = \" itâ???Ts \", replacement = \" \")\n\n# once again, check the result of the transformation\nstrwrap(docs[1:20])\n\n# Update DTM\ndtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(dtm_minimum_word_length, dtm_maximum_word_length), bounds = list(global = c(dtm_minimum_doc_frequency,dtm_maximum_doc_frequency))))\n# Count number of terms\ndtmr\n\n# get frequency of words in the corpus\nfreqr <- colSums(as.matrix(dtmr))\n# freq\n\n#length should be total number of terms\nlength(freqr)\n\n# create sort order (descending)\nordr <- order(freqr,decreasing=TRUE)\n# ord\n\n#inspect most frequently occurring terms\nfreqr[head(ordr)]\n\n#inspect least frequently occurring terms\nfreqr[tail(ordr)]\n\n# let's take get a list of terms that occur at least a  100 times in the entire corpus\nfindFreqTerms(dtmr,lowfreq=100)\n\n# term correlations: correlation is a quantitative measure of the co-occurrence of words in multiple documents\ncorrelation_threshold <- .15\nfindAssocs(dtmr,\"northwesternuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"universityofmichiganannarbor\",correlation_threshold)\nfindAssocs(dtmr,\"universityofwisconsinmadison\",correlation_threshold)\nfindAssocs(dtmr,\"universityofillinoisaturbanachampaign\",correlation_threshold)\nfindAssocs(dtmr,\"purdueuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"pennsylvaniastateuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"ohiostateuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"universityofminnesotatwincities\",correlation_threshold)\nfindAssocs(dtmr,\"michiganstateuniversity\",correlation_threshold)\nfindAssocs(dtmr,\"universityofmarylandcollegepark\",correlation_threshold)\nfindAssocs(dtmr,\"indianauniversitybloomington\",correlation_threshold)\nfindAssocs(dtmr,\"universityofiowa\",correlation_threshold)\nfindAssocs(dtmr,\"rutgersuniversitynewbrunswick\",correlation_threshold)\nfindAssocs(dtmr,\"universityofnebraskalincoln\",correlation_threshold)\n\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# create plot of top ten most used words\ndtm.matrix <- as.matrix(dtm)\nwordcount <- colSums(dtm.matrix)\ntopten <- head(sort(wordcount, decreasing=TRUE), 10)\ntoptwenty <- head(sort(wordcount, decreasing=TRUE), 20)\n\ndfplot <- as.data.frame(melt(topten))\ndfplot$word <- dimnames(dfplot)[[1]]\ndfplot$word <- factor(dfplot$word, levels=dfplot$word[order(dfplot$value,decreasing=TRUE)])\n\nfig <- ggplot(dfplot, aes(x=word, y=value)) + geom_bar(stat=\"identity\")\nfig <- fig + xlab(\"Word in Corpus\")\nfig <- fig + ylab(\"Count\")\nprint(fig)\n\nlibrary(ngram)\n\nn_count = 4\ninstitutions_vector = c('northwesternuniversity','universityofmichiganannarbor','universityofwisconsinmadison','universityofillinoisaturbanachampaign','purdueuniversity','pennsylvaniastateuniversity','ohiostateuniversity','universityofminnesotatwincities','michiganstateuniversity','universityofmarylandcollegepark','indianauniversitybloomington','universityofiowa','rutgersuniversitynewbrunswick','universityofnebraskalincoln')\nfor( i in 1:length(institutions_vector)) {\n  for( current_n in 2:n_count) {\n    print(paste(\"--- Begin N-Gram Analysis (n=\", n_count, \") for \", institutions_vector[i], \" --- \"))\n    \n    # subset corpus to create a smaller corpus of just nebraska reviews\n    institution_corpus <-\n      tm_filter(docs, function(x)\n        any(grep(institutions_vector[i], x, fixed = TRUE)))\n    # strip institution id out of corpus since we know what institution we are doing n-gram analysis on to prevent institution name from showing up in results\n    institution_corpus <-\n      tm_map(\n        institution_corpus,\n        content_transformer(gsub),\n        pattern = institutions_vector[i],\n        replacement = \" \"\n      )\n    \n    # institution_corpus\n    # view first row in reduced corpus\n    print(paste(\"--- Begin N-Gram Analysis (n = \", current_n , \") for \",  institutions_vector[i], \" --- \"))\n    # create ngram object\n    institution_ng <-\n      ngram(concatenate(lapply(institution_corpus, \"[\", 1)), n = current_n)\n    \n    # print top 5 phrasetable n-grams found in university reviews\n    print(head(get.phrasetable(institution_ng), 5))\n    \n    print(paste(\"--- End N-Gram Analysis (n = \", current_n , \") for \", institutions_vector[i], \" --- \"))\n    # End N-Gram Analyis\n    \n    \n    addtl_words_to_filter <- c(\"feel\", \"opportunities\", \"student\", \"campus\", \"school\", \"people\", \"great\", \"many\", \"college\", \"always\", \"state\", \"love\", \"place\", \"professor\", \"great\")\n    \n    institution_corpus <- tm_map(institution_corpus, removeWords, addtl_words_to_filter)\n    \n    # Here we have told R to include all those words \n    # We have also enforced  lower and upper limit to length of the words\n    institution_dtmr <-\n      DocumentTermMatrix(institution_corpus, control = list(wordLengths = c(dtm_minimum_word_length, dtm_maximum_word_length), bounds = list(global = c(dtm_minimum_doc_frequency, dtm_maximum_doc_frequency))))\n    # institution_dtmr\n    \n    # get frequency of words in the corpus\n    freq <- colSums(as.matrix(institution_dtmr))\n    # freq\n    \n    #length should be total number of terms\n    # length(freq)\n    \n    # create sort order (descending)\n    institution_ord <- order(freq, decreasing = TRUE)\n    institution_ord\n    institution_freqr <- colSums(as.matrix(institution_dtmr))\n    \n    #length should be total number of terms\n    length(institution_freqr)\n    \n    #inspect most frequently occurring terms\n    institution_dtm.matrix <- as.matrix(institution_dtmr)\n    wordcount <- colSums(institution_dtm.matrix)\n    toptwenty <- head(sort(wordcount, decreasing=TRUE), 20)\n    if(current_n == 2){\n      print(paste('--- Most Common Terms for ', institutions_vector[i], \" --- \"))\n      print(toptwenty)\n    }\n    \n    #WORDCLOUD\n    \n    library(wordcloud)\n    # setting the same seed each time ensures consistent look across clouds\n    set.seed(42)\n    layout(matrix(c(1, 2), nrow = 2), heights = c(1, 4))\n    par(mar = rep(0, 4))\n    plot.new()\n    text(x = 0.5, y = 0.5, institutions_vector[i])\n\n    # color wordcloud with 10 or more occurances within the instituion's dtm\n    wordcloud(\n      names(institution_freqr),\n      institution_freqr,\n      min.freq = 10,\n      colors = brewer.pal(6, \"Dark2\"),\n      main = \"Title\"\n    )\n    \n    # color wordcloud with 5 or more occurances within the instituion's ngram\n    layout(matrix(c(1, 2), nrow = 2), heights = c(1, 4))\n    par(mar = rep(0, 4))\n    plot.new()\n    text(x = 0.5, y = 0.5, institutions_vector[i])\n    wordcloud(\n      get.phrasetable(institution_ng)$ngram,\n      get.phrasetable(institution_ng)$freq,\n      min.freq = 5,\n      max.words = 25,\n      random.order = F,\n      colors = brewer.pal(6, \"Dark2\"),\n      main = \"Title\"\n    )\n    \n  }\n}\n\n\n\naddtl_words_to_filter <- c(\"feel\", \"opportunities\", \"student\", \"campus\", \"school\", \"people\", \"great\", \"many\", \"college\", \"always\", \"state\", \"love\", \"place\", \"professor\", \"great\")\n\ndocs_2 <- tm_map(docs, removeWords, addtl_words_to_filter)\ndocs_2\n\ndtmr2 <-DocumentTermMatrix(docs_2, control=list(wordLengths=c(dtm_minimum_word_length, dtm_maximum_word_length), bounds = list(global = c(dtm_minimum_doc_frequency,dtm_maximum_doc_frequency))))\n# Count number of terms\ndtmr2\n\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# create plot of top ten most used words\ndtmr2.matrix <- as.matrix(dtmr2)\nwordcount <- colSums(dtmr2.matrix)\ntopten <- head(sort(wordcount, decreasing=TRUE), 10)\ntoptwenty <- head(sort(wordcount, decreasing=TRUE), 20)\n\ndfplot <- as.data.frame(melt(topten))\ndfplot$word <- dimnames(dfplot)[[1]]\ndfplot$word <- factor(dfplot$word, levels=dfplot$word[order(dfplot$value,decreasing=TRUE)])\n\nfig <- ggplot(dfplot, aes(x=word, y=value)) + geom_bar(stat=\"identity\")\nfig <- fig + xlab(\"Word in Corpus\")\nfig <- fig + ylab(\"Count\")\nprint(fig)\n",
    "created" : 1518406547735.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "2464791660",
    "id" : "5CC31B89",
    "lastKnownWriteTime" : 4294967302,
    "last_content_update" : 1518407658801,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled4"
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}